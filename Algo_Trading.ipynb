{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f1a08bd",
   "metadata": {},
   "source": [
    "## Mean-reversion-based pairs trading strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa4f96",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d202d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e6b28f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetches S&P 500 tickers from Wikipedia\n",
    "def get_sp500_tickers():\n",
    "\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "    tables = pd.read_html(url)\n",
    "    sp500_table = tables[0]\n",
    "    tickers = sp500_table['Symbol'].tolist()\n",
    "    \n",
    "    # Clean tickers\n",
    "    tickers = [ticker.replace('.', '-') for ticker in tickers]\n",
    "    \n",
    "    print(f\"Found {len(tickers)} S&P 500 stocks\")\n",
    "    return tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4effa4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads historical data\n",
    "def download_stock_data(tickers, start_date='2022-01-01', end_date='2024-12-31'):\n",
    "\n",
    "    print(f\"Downloading data from {start_date} to {end_date}.\")\n",
    "    \n",
    "    # Downloads data in batches\n",
    "    all_data = {}\n",
    "    failed_tickers = []\n",
    "    \n",
    "    for i, ticker in enumerate(tickers):\n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            hist = stock.history(start=start_date, end=end_date)\n",
    "            \n",
    "            if len(hist) > 0:\n",
    "                all_data[ticker] = hist['Close']  # Adjusted close\n",
    "            else:\n",
    "                failed_tickers.append(ticker)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {ticker}: {e}\")\n",
    "            failed_tickers.append(ticker)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Downloaded {i + 1}/{len(tickers)} stocks...\")\n",
    "    \n",
    "    # Creates DataFrame\n",
    "    price_data = pd.DataFrame(all_data)\n",
    "    \n",
    "    print(f\"Successfully downloaded {len(price_data.columns)} stocks\")\n",
    "    print(f\"Failed to download {len(failed_tickers)} stocks: {failed_tickers[:10]}...\")\n",
    "    \n",
    "    return price_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc34a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 503 S&P 500 stocks\n",
      "Downloading data from 2022-01-01 to 2024-12-31.\n",
      "Downloaded 50/503 stocks...\n",
      "Downloaded 100/503 stocks...\n",
      "Downloaded 150/503 stocks...\n",
      "Downloaded 200/503 stocks...\n",
      "Downloaded 250/503 stocks...\n",
      "Downloaded 300/503 stocks...\n",
      "Downloaded 350/503 stocks...\n",
      "Downloaded 400/503 stocks...\n",
      "Downloaded 450/503 stocks...\n",
      "Downloaded 500/503 stocks...\n",
      "Successfully downloaded 503 stocks\n",
      "Failed to download 0 stocks: []...\n",
      "  DATA CLEANING  \n",
      "Original data shape: (752, 503)\n",
      "Date range: 2022-01-03 00:00:00-05:00 to 2024-12-30 00:00:00-05:00\n",
      "Initial missing data points: 2779\n",
      "\n",
      "1. Filtering stocks with <95.0% data availability...\n",
      "   Kept 497 stocks\n",
      "   Removed 6 stocks\n",
      "   Missing data points after stock filter: 11\n",
      "\n",
      "2. Handling missing data...\n",
      "   Missing data points before fill: 11\n",
      "   Missing data points after fill: 0\n",
      "\n",
      "3. Filtering trading days...\n",
      "   Maximum allowed missing stocks per day: 99\n",
      "   Removed 0 days with too many missing stocks\n",
      "   Missing data points after day filter: 0\n",
      "\n",
      "=== FINAL RESULTS ===\n",
      "Final data shape: (752, 497)\n",
      "Final date range: 2022-01-03 00:00:00-05:00 to 2024-12-30 00:00:00-05:00\n",
      "Total missing values: 0\n",
      "Data completeness: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3q/h_xjj4xj0v79kyl_3hjsnh5h0000gp/T/ipykernel_25433/1320882902.py:23: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  filtered_data = filtered_data.fillna(method='ffill')\n",
      "/var/folders/3q/h_xjj4xj0v79kyl_3hjsnh5h0000gp/T/ipykernel_25433/1320882902.py:27: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  filtered_data = filtered_data.fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'sp500_prices_cleaned.csv'\n",
      "\n",
      "Final dataset summary:\n",
      "Number of stocks: 497\n",
      "Number of trading days: 752\n"
     ]
    }
   ],
   "source": [
    "def clean_data(price_data, min_data_threshold=0.95, min_trading_days=0.8):\n",
    "\n",
    "    print(\"  DATA CLEANING  \")\n",
    "    print(f\"Original data shape: {price_data.shape}\")\n",
    "    print(f\"Date range: {price_data.index[0]} to {price_data.index[-1]}\")\n",
    "    print(f\"Initial missing data points: {price_data.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Remove stocks with insufficient data\n",
    "    print(f\"\\n1. Filtering stocks with <{min_data_threshold*100}% data availability...\")\n",
    "    data_availability = price_data.count() / len(price_data)\n",
    "    good_stocks = data_availability[data_availability >= min_data_threshold].index\n",
    "    filtered_data = price_data[good_stocks].copy()\n",
    "    \n",
    "    print(f\"   Kept {len(good_stocks)} stocks\")\n",
    "    print(f\"   Removed {len(price_data.columns) - len(good_stocks)} stocks\")\n",
    "    print(f\"   Missing data points after stock filter: {filtered_data.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Handle missing data with forward fill\n",
    "    print(f\"\\n2. Handling missing data...\")\n",
    "    print(f\"   Missing data points before fill: {filtered_data.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Forward fill missing values \n",
    "    filtered_data = filtered_data.fillna(method='ffill')\n",
    "    \n",
    "    # If there are still missing values at the beginning, backward fill\n",
    "    if filtered_data.isnull().sum().sum() > 0:\n",
    "        filtered_data = filtered_data.fillna(method='bfill')\n",
    "    \n",
    "    print(f\"   Missing data points after fill: {filtered_data.isnull().sum().sum()}\")\n",
    "    \n",
    "    #  Remove days with too many missing stocks \n",
    "    print(f\"\\n3. Filtering trading days...\")\n",
    "    missing_per_day = filtered_data.isnull().sum(axis=1)\n",
    "    max_missing_stocks = int(len(filtered_data.columns) * (1 - min_trading_days))\n",
    "    \n",
    "    print(f\"   Maximum allowed missing stocks per day: {max_missing_stocks}\")\n",
    "    \n",
    "    # Keep days where we have at least 80% of stocks with data\n",
    "    valid_days = missing_per_day <= max_missing_stocks\n",
    "    final_data = filtered_data[valid_days].copy()\n",
    "    \n",
    "    removed_days = len(filtered_data) - len(final_data)\n",
    "    print(f\"   Removed {removed_days} days with too many missing stocks\")\n",
    "    print(f\"   Missing data points after day filter: {final_data.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Step 4: Final cleanup \n",
    "    completely_empty_rows = final_data.isnull().all(axis=1)\n",
    "    if completely_empty_rows.any():\n",
    "        final_data = final_data[~completely_empty_rows]\n",
    "        print(f\"   Removed {completely_empty_rows.sum()} completely empty days\")\n",
    "    \n",
    "    # Final statistics\n",
    "    print(f\"\\n=== FINAL RESULTS ===\")\n",
    "    print(f\"Final data shape: {final_data.shape}\")\n",
    "    print(f\"Final date range: {final_data.index[0]} to {final_data.index[-1]}\")\n",
    "    print(f\"Total missing values: {final_data.isnull().sum().sum()}\")\n",
    "    print(f\"Data completeness: {(1 - final_data.isnull().sum().sum() / (final_data.shape[0] * final_data.shape[1])) * 100:.2f}%\")\n",
    "    \n",
    "    return final_data\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Get S&P 500 tickers\n",
    "    sp500_tickers = get_sp500_tickers()\n",
    "    \n",
    "    # Download data\n",
    "    raw_data = download_stock_data(sp500_tickers)\n",
    "    \n",
    "    # Clean data\n",
    "    clean_price_data = clean_data(raw_data)\n",
    "    \n",
    "    # Save for later use\n",
    "    clean_price_data.to_csv('sp500_prices_cleaned.csv')\n",
    "    print(\"Data saved to 'sp500_prices_cleaned.csv'\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nFinal dataset summary:\")\n",
    "    print(f\"Number of stocks: {clean_price_data.shape[1]}\")\n",
    "    print(f\"Number of trading days: {clean_price_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9756dbfd",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d06838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data = pd.read_csv('./sp500_prices_cleaned.csv', index_col=0, parse_dates=True)\n",
    "price_data = price_data.select_dtypes(include=[np.number])  # Keep only numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff170443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trading_day_allignment(price_data):\n",
    "\n",
    "    print(\"Original data shape:\", price_data.shape)\n",
    "    \n",
    "    # Drop rows where ANY stock has NaN\n",
    "    aligned_data = price_data.dropna()\n",
    "    \n",
    "    print(\"After inner join alignment:\", aligned_data.shape)\n",
    "    print(f\"Removed {len(price_data) - len(aligned_data)} trading days\")\n",
    "    \n",
    "    return aligned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de6afaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (752, 497)\n",
      "After inner join alignment: (752, 497)\n",
      "Removed 0 trading days\n"
     ]
    }
   ],
   "source": [
    "align = trading_day_allignment(price_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5387c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_by_trading_days_common(price_data, min_stocks_threshold=0.8):\n",
    "\n",
    "    print(\"Original data shape:\", price_data.shape)\n",
    "    \n",
    "    # Count how many stocks have data each day\n",
    "    stocks_per_day = price_data.count(axis=1)\n",
    "    total_stocks = len(price_data.columns)\n",
    "    \n",
    "    # Keep days where at least 80% of stocks have data\n",
    "    min_stocks = int(total_stocks * min_stocks_threshold)\n",
    "    valid_days = stocks_per_day >= min_stocks\n",
    "    \n",
    "    aligned_data = price_data[valid_days].copy()\n",
    "    \n",
    "    print(f\"Kept days with ≥{min_stocks} stocks ({min_stocks_threshold*100}%)\")\n",
    "    print(\"After common trading days alignment:\", aligned_data.shape)\n",
    "    \n",
    "    return aligned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "321ad504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (752, 497)\n",
      "Kept days with ≥397 stocks (80.0%)\n",
      "After common trading days alignment: (752, 497)\n"
     ]
    }
   ],
   "source": [
    "align2 = align_by_trading_days_common(price_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49430eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_price_transformations(price_data):\n",
    "    \n",
    "    # 1. Raw prices \n",
    "    raw_prices = price_data.copy()\n",
    "    \n",
    "    # 2. Log transformation\n",
    "    log_prices = np.log(price_data)\n",
    "    \n",
    "    # 3. Normalized prices (alternative approach)\n",
    "    normalized_prices = price_data / price_data.iloc[0]  # Normalize to first observation\n",
    "    \n",
    "    print(\"=== PRICE TRANSFORMATION COMPARISON ===\")\n",
    "    print(f\"Raw prices shape: {raw_prices.shape}\")\n",
    "    print(f\"Log prices shape: {log_prices.shape}\")\n",
    "    print(f\"Normalized prices shape: {normalized_prices.shape}\")\n",
    "    \n",
    "    # Check for any issues with log transformation\n",
    "    negative_prices = (price_data <= 0).sum().sum()\n",
    "    print(f\"Negative/zero prices (problematic for log): {negative_prices}\")\n",
    "    \n",
    "    return raw_prices, log_prices, normalized_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "463dc805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PRICE TRANSFORMATION COMPARISON ===\n",
      "Raw prices shape: (752, 497)\n",
      "Log prices shape: (752, 497)\n",
      "Normalized prices shape: (752, 497)\n",
      "Negative/zero prices (problematic for log): 0\n"
     ]
    }
   ],
   "source": [
    "transformation = compare_price_transformations(price_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1583441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock1= price_data['AAPL']\n",
    "stock2 = price_data['GOOG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "089cc64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_cointegration_with_transformations(stock1, stock2, method=\"raw\"):\n",
    "\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    from statsmodels.api import OLS\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    if method == \"log\":\n",
    "        s1, s2 = np.log(stock1), np.log(stock2)\n",
    "        print(\"Testing LOG PRICES for cointegration\")\n",
    "    elif method == \"normalized\":\n",
    "        s1, s2 = stock1/stock1.iloc[0], stock2/stock2.iloc[0]\n",
    "        print(\"Testing NORMALIZED PRICES for cointegration\")\n",
    "    else:\n",
    "        s1, s2 = stock1, stock2\n",
    "        print(\"Testing RAW PRICES for cointegration\")\n",
    "    \n",
    "    # Linear regression\n",
    "    s1_clean = s1.dropna()\n",
    "    s2_clean = s2.dropna()\n",
    "    \n",
    "    # Align the series\n",
    "    common_idx = s1_clean.index.intersection(s2_clean.index)\n",
    "    s1_aligned = s1_clean[common_idx]\n",
    "    s2_aligned = s2_clean[common_idx]\n",
    "    \n",
    "    # Regression: s2 = alpha + beta * s1 + error\n",
    "    X = sm.add_constant(s1_aligned)\n",
    "    model = OLS(s2_aligned, X).fit()\n",
    "    hedge_ratio = model.params[1]\n",
    "    \n",
    "    # Create spread and test for stationarity\n",
    "    spread = s2_aligned - hedge_ratio * s1_aligned\n",
    "    \n",
    "    # ADF test on spread (as per slide 8-9)\n",
    "    adf_result = adfuller(spread.dropna())\n",
    "    \n",
    "    print(f\"Hedge ratio (beta): {hedge_ratio:.4f}\")\n",
    "    print(f\"ADF test statistic: {adf_result[0]:.4f}\")\n",
    "    print(f\"ADF p-value: {adf_result[1]:.4f}\")\n",
    "    print(f\"Cointegrated (p < 0.05): {adf_result[1] < 0.05}\")\n",
    "    \n",
    "    return hedge_ratio, adf_result[1], spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5013960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LOG PRICES for cointegration\n",
      "Hedge ratio (beta): 1.1212\n",
      "ADF test statistic: -2.4012\n",
      "ADF p-value: 0.1414\n",
      "Cointegrated (p < 0.05): False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3q/h_xjj4xj0v79kyl_3hjsnh5h0000gp/T/ipykernel_25433/3799395186.py:29: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  hedge_ratio = model.params[1]\n"
     ]
    }
   ],
   "source": [
    "test_log = test_cointegration_with_transformations(stock1, stock2, method=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "701b8121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing NORMALIZED PRICES for cointegration\n",
      "Hedge ratio (beta): 1.0131\n",
      "ADF test statistic: -2.4368\n",
      "ADF p-value: 0.1316\n",
      "Cointegrated (p < 0.05): False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3q/h_xjj4xj0v79kyl_3hjsnh5h0000gp/T/ipykernel_25433/3799395186.py:29: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  hedge_ratio = model.params[1]\n"
     ]
    }
   ],
   "source": [
    "test_normalized = test_cointegration_with_transformations(stock1, stock2, method=\"normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23148bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAW PRICES for cointegration\n",
      "Hedge ratio (beta): 0.8178\n",
      "ADF test statistic: -2.4368\n",
      "ADF p-value: 0.1316\n",
      "Cointegrated (p < 0.05): False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3q/h_xjj4xj0v79kyl_3hjsnh5h0000gp/T/ipykernel_25433/3799395186.py:29: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  hedge_ratio = model.params[1]\n"
     ]
    }
   ],
   "source": [
    "test = test_cointegration_with_transformations(stock1,stock2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0189d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "def get_sectors_automatically(stock_tickers, batch_size=50):\n",
    "  \n",
    "    print(f\"Classifying {len(stock_tickers)} stocks by sector...\")\n",
    "    \n",
    "    sectors = defaultdict(list)\n",
    "    failed_tickers = []\n",
    "    \n",
    "    for i, ticker in enumerate(stock_tickers):\n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            info = stock.info\n",
    "            \n",
    "            sector = info.get('sector', 'Unknown')\n",
    "            industry = info.get('industry', 'Unknown')\n",
    "            \n",
    "            # Clean up sector names\n",
    "            if sector and sector != 'Unknown':\n",
    "                sectors[sector].append(ticker)\n",
    "            else:\n",
    "                sectors['Unknown'].append(ticker)\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (i + 1) % batch_size == 0:\n",
    "                print(f\"Processed {i + 1}/{len(stock_tickers)} stocks...\")\n",
    "                \n",
    "            # Rate limiting to avoid API limits\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get sector for {ticker}: {e}\")\n",
    "            failed_tickers.append(ticker)\n",
    "            sectors['Unknown'].append(ticker)\n",
    "    \n",
    "    # Convert to regular dict and show results\n",
    "    sectors_dict = dict(sectors)\n",
    "    \n",
    "    print(f\"\\n=== SECTOR CLASSIFICATION RESULTS ===\")\n",
    "    for sector, stocks in sectors_dict.items():\n",
    "        print(f\"{sector}: {len(stocks)} stocks\")\n",
    "        if len(stocks) <= 10:  # Show stocks for smaller sectors\n",
    "            print(f\"  Stocks: {stocks}\")\n",
    "    \n",
    "    print(f\"\\nFailed to classify: {len(failed_tickers)} stocks\")\n",
    "    \n",
    "    return sectors_dict, failed_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99d988ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stock tickers from your price_data DataFrame\n",
    "stock_tickers = price_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9f15594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying 497 stocks by sector...\n",
      "Processed 50/497 stocks...\n",
      "Processed 100/497 stocks...\n",
      "Processed 150/497 stocks...\n",
      "Processed 200/497 stocks...\n",
      "Processed 250/497 stocks...\n",
      "Processed 300/497 stocks...\n",
      "Processed 350/497 stocks...\n",
      "Processed 400/497 stocks...\n",
      "Processed 450/497 stocks...\n",
      "\n",
      "=== SECTOR CLASSIFICATION RESULTS ===\n",
      "Industrials: 69 stocks\n",
      "Healthcare: 59 stocks\n",
      "Technology: 82 stocks\n",
      "Utilities: 31 stocks\n",
      "Financial Services: 68 stocks\n",
      "Basic Materials: 20 stocks\n",
      "Consumer Cyclical: 55 stocks\n",
      "Real Estate: 31 stocks\n",
      "Communication Services: 23 stocks\n",
      "Consumer Defensive: 36 stocks\n",
      "Energy: 23 stocks\n",
      "\n",
      "Failed to classify: 0 stocks\n"
     ]
    }
   ],
   "source": [
    "# Then run the sector classification\n",
    "sectors_dict, failed_tickers = get_sectors_automatically(stock_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73add52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cointegration_silent(stock1, stock2, method=\"log\"):\n",
    "    \"\"\"\n",
    "    Modified version of your function for batch processing\n",
    "    Returns: (p_value, hedge_ratio, adf_statistic, spread)\n",
    "    \"\"\"\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    from statsmodels.api import OLS\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    try:\n",
    "        if method == \"log\":\n",
    "            s1, s2 = np.log(stock1), np.log(stock2)\n",
    "        elif method == \"normalized\":\n",
    "            s1, s2 = stock1/stock1.iloc[0], stock2/stock2.iloc[0]\n",
    "        else:\n",
    "            s1, s2 = stock1, stock2\n",
    "        \n",
    "        # Linear regression\n",
    "        s1_clean = s1.dropna()\n",
    "        s2_clean = s2.dropna()\n",
    "        \n",
    "        # Align the series\n",
    "        common_idx = s1_clean.index.intersection(s2_clean.index)\n",
    "        s1_aligned = s1_clean[common_idx]\n",
    "        s2_aligned = s2_clean[common_idx]\n",
    "        \n",
    "        # Need sufficient data points\n",
    "        if len(common_idx) < 60:\n",
    "            return None, None, None, None\n",
    "        \n",
    "        # Regression: s2 = alpha + beta * s1 + error\n",
    "        X = sm.add_constant(s1_aligned)\n",
    "        model = OLS(s2_aligned, X).fit()\n",
    "        hedge_ratio = model.params.iloc[1]  # Using iloc to avoid the warning\n",
    "        \n",
    "        # Create spread and test for stationarity\n",
    "        spread = s2_aligned - hedge_ratio * s1_aligned\n",
    "        \n",
    "        # ADF test on spread\n",
    "        adf_result = adfuller(spread.dropna())\n",
    "        \n",
    "        return adf_result[1], hedge_ratio, adf_result[0], spread\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02e58633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all_sector_pairs_with_your_function(sector_data, sector_name, method=\"log\"):\n",
    "\n",
    "    print(f\"\\nTesting all pairs in {sector_name} sector using {method} prices...\")\n",
    "    \n",
    "    results = []\n",
    "    stock_list = sector_data.columns.tolist()\n",
    "    \n",
    "    if len(stock_list) < 2:\n",
    "        print(f\"Insufficient stocks in {sector_name} sector\")\n",
    "        return results\n",
    "    \n",
    "    total_pairs = len(stock_list) * (len(stock_list) - 1) // 2\n",
    "    print(f\"Testing {total_pairs} pairs from {len(stock_list)} stocks...\")\n",
    "    \n",
    "    pair_count = 0\n",
    "    \n",
    "    for i, stock_a in enumerate(stock_list):\n",
    "        for j, stock_b in enumerate(stock_list[i+1:], i+1):\n",
    "            pair_count += 1\n",
    "            \n",
    "            # Test both directions using your function\n",
    "            p_val_1, hedge_1, adf_1, spread_1 = test_cointegration_silent(\n",
    "                sector_data[stock_a], sector_data[stock_b], method=method\n",
    "            )\n",
    "            \n",
    "            p_val_2, hedge_2, adf_2, spread_2 = test_cointegration_silent(\n",
    "                sector_data[stock_b], sector_data[stock_a], method=method\n",
    "            )\n",
    "            \n",
    "            # Keep the better result (more negative ADF statistic)\n",
    "            if p_val_1 is not None and p_val_2 is not None:\n",
    "                if adf_1 < adf_2:  # More negative is better\n",
    "                    best_p_val = p_val_1\n",
    "                    best_hedge = hedge_1\n",
    "                    best_adf = adf_1\n",
    "                    direction = f\"{stock_b} ~ {stock_a}\"\n",
    "                    spread = spread_1\n",
    "                    dependent_var = stock_b\n",
    "                    independent_var = stock_a\n",
    "                else:\n",
    "                    best_p_val = p_val_2\n",
    "                    best_hedge = hedge_2\n",
    "                    best_adf = adf_2\n",
    "                    direction = f\"{stock_a} ~ {stock_b}\"\n",
    "                    spread = spread_2\n",
    "                    dependent_var = stock_a\n",
    "                    independent_var = stock_b\n",
    "                \n",
    "                # Only keep statistically significant pairs\n",
    "                if best_p_val < 0.05:\n",
    "                    results.append({\n",
    "                        'stock_a': independent_var,\n",
    "                        'stock_b': dependent_var,\n",
    "                        'p_value': best_p_val,\n",
    "                        'adf_statistic': best_adf,\n",
    "                        'hedge_ratio': best_hedge,\n",
    "                        'direction': direction,\n",
    "                        'sector': sector_name,\n",
    "                        'method': method,\n",
    "                        'spread': spread,\n",
    "                        'economic_rationale': f\"Both operate in {sector_name} sector\"\n",
    "                    })\n",
    "            \n",
    "            # Progress indicator\n",
    "            if pair_count % 50 == 0:\n",
    "                print(f\"  Tested {pair_count}/{total_pairs} pairs...\")\n",
    "    \n",
    "    cointegrated_count = len(results)\n",
    "    print(f\"Found {cointegrated_count} cointegrated pairs in {sector_name}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30090f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing all pairs in Technology sector using log prices...\n",
      "Testing 3321 pairs from 82 stocks...\n",
      "  Tested 50/3321 pairs...\n",
      "  Tested 100/3321 pairs...\n",
      "  Tested 150/3321 pairs...\n",
      "  Tested 200/3321 pairs...\n",
      "  Tested 250/3321 pairs...\n",
      "  Tested 300/3321 pairs...\n",
      "  Tested 350/3321 pairs...\n",
      "  Tested 400/3321 pairs...\n",
      "  Tested 450/3321 pairs...\n",
      "  Tested 500/3321 pairs...\n",
      "  Tested 550/3321 pairs...\n",
      "  Tested 600/3321 pairs...\n",
      "  Tested 650/3321 pairs...\n",
      "  Tested 700/3321 pairs...\n",
      "  Tested 750/3321 pairs...\n",
      "  Tested 800/3321 pairs...\n",
      "  Tested 850/3321 pairs...\n",
      "  Tested 900/3321 pairs...\n",
      "  Tested 950/3321 pairs...\n",
      "  Tested 1000/3321 pairs...\n",
      "  Tested 1050/3321 pairs...\n",
      "  Tested 1100/3321 pairs...\n",
      "  Tested 1150/3321 pairs...\n",
      "  Tested 1200/3321 pairs...\n",
      "  Tested 1250/3321 pairs...\n",
      "  Tested 1300/3321 pairs...\n",
      "  Tested 1350/3321 pairs...\n",
      "  Tested 1400/3321 pairs...\n",
      "  Tested 1450/3321 pairs...\n",
      "  Tested 1500/3321 pairs...\n",
      "  Tested 1550/3321 pairs...\n",
      "  Tested 1600/3321 pairs...\n",
      "  Tested 1650/3321 pairs...\n",
      "  Tested 1700/3321 pairs...\n",
      "  Tested 1750/3321 pairs...\n",
      "  Tested 1800/3321 pairs...\n",
      "  Tested 1850/3321 pairs...\n",
      "  Tested 1900/3321 pairs...\n",
      "  Tested 1950/3321 pairs...\n",
      "  Tested 2000/3321 pairs...\n",
      "  Tested 2050/3321 pairs...\n",
      "  Tested 2100/3321 pairs...\n",
      "  Tested 2150/3321 pairs...\n",
      "  Tested 2200/3321 pairs...\n",
      "  Tested 2250/3321 pairs...\n",
      "  Tested 2300/3321 pairs...\n",
      "  Tested 2350/3321 pairs...\n",
      "  Tested 2400/3321 pairs...\n",
      "  Tested 2450/3321 pairs...\n",
      "  Tested 2500/3321 pairs...\n",
      "  Tested 2550/3321 pairs...\n",
      "  Tested 2600/3321 pairs...\n",
      "  Tested 2650/3321 pairs...\n",
      "  Tested 2700/3321 pairs...\n",
      "  Tested 2750/3321 pairs...\n",
      "  Tested 2800/3321 pairs...\n",
      "  Tested 2850/3321 pairs...\n",
      "  Tested 2900/3321 pairs...\n",
      "  Tested 2950/3321 pairs...\n",
      "  Tested 3000/3321 pairs...\n",
      "  Tested 3050/3321 pairs...\n",
      "  Tested 3100/3321 pairs...\n",
      "  Tested 3150/3321 pairs...\n",
      "  Tested 3200/3321 pairs...\n",
      "  Tested 3250/3321 pairs...\n",
      "  Tested 3300/3321 pairs...\n",
      "Found 1278 cointegrated pairs in Technology\n"
     ]
    }
   ],
   "source": [
    "# Example for Technology sector:\n",
    "tech_stocks = sectors_dict['Technology']  # List from Step 1\n",
    "available_tech_stocks = [s for s in tech_stocks if s in price_data.columns]\n",
    "sector_data = price_data[available_tech_stocks]  # DataFrame with only tech stocks\n",
    "\n",
    "# Input 2: sector_name (String)\n",
    "sector_name = \"Technology\"\n",
    "\n",
    "# Input 3: method (String, optional)\n",
    "method = \"log\"  # or \"raw\" or \"normalized\"\n",
    "\n",
    "# Usage:\n",
    "tech_results = test_all_sector_pairs_with_your_function(sector_data, sector_name, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf83239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
